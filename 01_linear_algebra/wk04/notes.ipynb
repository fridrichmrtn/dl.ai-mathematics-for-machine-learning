{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinants and eigen-vectors\n",
    "\n",
    "### Determinants\n",
    "\n",
    "If linear transformation from one basis to another covers:  \n",
    "* the whole plane (in parallelogram), the transformation is non-singular\n",
    "* line (degenerate parallelogram), the transformation is singular\n",
    "* point, the transformation is singular\n",
    "\n",
    "Dimension of the result dependent on the rank of the transfornmation matrix (eg plane - 2, line - 1, point  -0).\n",
    "\n",
    "Geometric properties  \n",
    "* if we have a unit square as basis and transform it using non-singular matrix, the area of the resulting parallelogram is equal to determinant\n",
    "* this is also true for singular matrices, as the line or point area is 0\n",
    "* negative determinant means transformation of the vectors in the counter-clockwise order, positive determinant means transformation the in clockwise order\n",
    "\n",
    "Dot product properties  \n",
    "* $det(AB) = det(A)det(B)$\n",
    "* geometrically, it blows up the area of the resulting parallelograms (if non-singular)\n",
    "\n",
    "Inverse properties  \n",
    "* $det(A^{-1}) = \\frac{1}{det(A)}$\n",
    "* this can be deduced from the dot product properties above, and calculate determinant of identity $det(I)$\n",
    "\n",
    "### Eigen-values and eigen-vectors\n",
    "\n",
    "* any two vector who are not parallel can form plane basis,\n",
    "\n",
    "* span\n",
    "    * what part of plane can be traversed using the vector directions\n",
    "    * two non-parallel vectors - plane, two parallel vectors/one vector - line\n",
    "\n",
    "* basis have minimal spanning set (minimal number of vectors needed based on dimensions -> 2 for plane, 1 for line etc)\n",
    "\n",
    "* basis is formed through linearly independent vectors, where the number of vectors needed equals the number of dimensions of the space (spans a vector space), every other vector then can be formed as linear combination of the basis ones\n",
    "\n",
    "Example:  \n",
    "$$\n",
    "basis: v_1 = \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} v_2 = \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix}\\\\\n",
    "~\\\\\n",
    "v_3 = \\begin{bmatrix} -5\\\\ 3 \\end{bmatrix}\\\\\n",
    "$$\n",
    "<hr/>\n",
    "\n",
    "$$\n",
    "\n",
    "\\alpha v_1 + \\beta v_2 = v_3\\\\\n",
    "~\\\\\n",
    "\\alpha \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} + \\beta \\begin{bmatrix} 2\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -5\\\\ 3 \\end{bmatrix}\\\\\n",
    "~\\\\\n",
    "\\alpha = \\frac{11}{3}  \\:\\:\\: \\beta = -\\frac{2}{3}\n",
    "$$\n",
    "<hr/>\n",
    "\n",
    "* eigen-bases - special case of transformation which sends parallelogram from original space to parallelogram in the new space, with sides parallel even to the original one (just stretching space in two dimensions), so the transformation can be expressed just by the vectors in the basis (eigen-vectors) and the stretching (eigen-values)\n",
    "\n",
    "* for eigen-vectors, linear transformation through the parent matrix (matrix multiplication) can be expressed as a product of eigen-values and eigen-vectors (scalar multiplication)\n",
    "\n",
    "* $ Av = \\lambda v$ for each eigen-vector v and eigen-value lambda\n",
    "\n",
    "* the practical application is based on expressing vector to be transformed as linear combination of eigen-vectors in the original space and then solving the transformation to the new space by scalar multiplication with eigen-values\n",
    "\n",
    "Calculation of eigen-vectors v and eigen-values lambda:  \n",
    "* solve characteristic polynomial $ Det((A-\\lambda I)) = 0 $\n",
    "    * eigen-values can repeat themselves (corresponding eigen-vectors might or might not form eigen-basis)\n",
    "* solve $ Av = \\lambda v$\n",
    "    * there are inf eigen vectors, thus pick simple representation (only direction matters)\n",
    "\n",
    "\n",
    "### Principal component analysis\n",
    "\n",
    "* dimensionality reduction is used to preserve informatio (spread) but reduce number of columns (easier to manage, faster downstream computation, visualization)\n",
    "\n",
    "* projecting a matrix A onto a vector v is done like $A_p = A \\frac{v}{||v||_2}$ (note the rescaling to the vector's 2nd norm)\n",
    "* this can be further generalized to projection done by matrix of projection vectors V $A_p = AV$ (number of inner dimensions must correspond, ie every vector v is transforming every column in A)\n",
    "\n",
    "Covariance matrix  \n",
    "* $ C =  \\begin{bmatrix} Var(x) & Cov(x, y)\\\\\n",
    "    Cov(y,x) & Var(y) \\end{bmatrix} $\n",
    "    \n",
    "* $ C = \\frac{1}{n-1}(A-\\mu)^T(A-\\mu) $\n",
    "    * arrange data with different feature in each column\n",
    "    * calc column means\n",
    "    * subtract each eam from column to generate $A - \\mu$\n",
    "\n",
    "* C is symmetric so its eigen-vectors are orthogonal (principal components!)\n",
    "\n",
    "PCA algorithm  \n",
    "* X (observations x features)\n",
    "* center X (along features)\n",
    "* calculate covariance matrix\n",
    "* calculate eigen-values and eigen-vectors, sort them from largest to smallest eigen-values\n",
    "* create projection matrix from the eigen-vectors ie $ V = \\begin{bmatrix} \\frac{v_1}{||v_1||_2} & \\frac{v_2}{||v_2||_2} & ... \\end{bmatrix} $\n",
    "\n",
    "### Textbooks and resources\n",
    "\n",
    "### References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
