{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population and sample\n",
    "\n",
    "* complete (population) vs incomplete (sample) set of observations\n",
    "* a sample must be representative of a population\n",
    "* sample vs population variance and mean\n",
    "* law of the large numbers - sample characteristics close to population ones if\n",
    "    * sample is randomly drawn,\n",
    "    * sample size is sufficiently large,\n",
    "    * observations are independent.\n",
    "\n",
    "**Central limit theorem**\n",
    "\n",
    "* sampling $E[X]$ from any distribution results into normal distribution as number of samples increases\n",
    "* As $n \\rightarrow \\infty \\frac{\\frac{1}{n}\\sum_{i=1}^{n} X_i-E[X]}{\\sigma_X}\\sqrt n \\sim N(0,1^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Point estimation\n",
    "\n",
    "**Maximum likelihood estimation**\n",
    "* probability - possibility of something happening, likelihood - adherence of a distribution to collected evidence\n",
    "* selecting the most likely scenario based on evidence collected (maximization of conditional probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLE for Gaussian population**\n",
    "\n",
    "In the videos, you got an intuition of what the Maximum Likelihood Estimation (MLE) should look like for the mean and variance of a Gaussian population. In this reading item, you will learn the derivation of both results.\n",
    "\n",
    "Suppose you have $n$ samples $X=(X_1,X_2,...,X_n)$ from a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$. This means that $X_i \\sim N(\\mu, \\sigma^2)$, where $X_i$ follow IID requirements.\n",
    "\n",
    "If you want hte MLE for $\\mu$ and $\\sigma$ the first step is to define the likelihood. If both $\\mu$ and $\\sigma$ are unknown, then the likelihood will be a function of these two parameters. For a realization fo $X$, given by $x=(x_1,x_2,...,x_n)$>\n",
    "$$\n",
    "\n",
    "L(\\mu,\\sigma;x) = \\prod_{i=1}^n f_{X_i}(x_I) = \\prod_{i=1}^n  \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2}\\frac{(x_i-\\mu)^2}{\\sigma^2}}\\\\\n",
    "= \\frac{1}{\\sqrt{2\\pi}^n\\sigma^n}e^{-\\frac{1}{2}\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Now all what is left to do is find the values of $\\mu$ and $\\sigma$ that maximize the likelihood $L(\\mu,\\sigma;x)$. Extremes of the likelihood function can be found through the equating its first derivative to zero. To simplify the procedure, it is beneficial to take a logarithm of the likelihood function (the log function is always increasing so they have same max). The log-likelihood is then defined as $l(\\mu,\\sigma)=log(L(\\mu,\\sigma;x))$.\n",
    "\n",
    "Some nice log properties refreshed here>\n",
    "$$\n",
    "log(a\\cdot b) = log(a)+log(b)\\\\\n",
    "log(1/a) = -log(a)\\\\\n",
    "log(a^k) = k \\cdot log(a)\\\\\n",
    "\\frac{d}{dx} (log_a(x))=\\frac{1}{x \\cdot ln(e)}\n",
    "$$\n",
    "\n",
    "Putting it all together>\n",
    "$$\n",
    "l(\\mu,\\sigma) = log(\\frac{1}{\\sqrt{2\\pi}^n\\sigma^n}e^{-\\frac{1}{2}\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^2}})\\\\\n",
    "= -\\frac{n}{2} log(2\\pi)-n\\cdot log(\\sigma)-\\frac{1}{2}\\frac{\\sum_{i=1}^n(x_i-\\mu)^2}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "Now, to find extremes for $\\mu$ adn $\\sigma$, we need to take the partial derivates of the log-likelihood, and equate them to zero. For the partial derivative with respect to $\\mu$ note that the first two terms do not involve $\\mu$ so we get>\n",
    "$$\n",
    "\\frac{\\delta}{\\delta\\mu}l(\\mu, \\sigma) = -\\frac{1}{2}\\frac{\\sum_{i=1}^n 2(x_i-\\mu)}{\\sigma^2}(-1)\\\\\n",
    "= \\frac{1}{\\sigma^2}(\\sum_{i=1}^n x_i-\\sum_{i=1}^n\\mu)\\\\\n",
    "= \\frac{1}{\\sigma^2}(\\sum_{i=1}^n x_i-n\\mu)\n",
    "$$\n",
    "For the partial derivative with respect to $\\sigma$ we get>\n",
    "$$\n",
    "\\frac{\\delta}{\\delta\\sigma}l(\\mu, \\sigma) = -\\frac{n}{\\sigma}-\\frac{1}{2}(\\sum_{i=1}^n (x_i-\\mu)^2)(-2)\\frac{1}{\\sigma^3}\\\\\n",
    "= -\\frac{n}{\\sigma}+(\\sum_{i=1}^n (x_i-\\mu)^2)\\frac{1}{\\sigma^3}\n",
    "$$\n",
    "\n",
    "Now let's examine the partial derivatives, please not that $\\sigma$>0>\n",
    "$$\n",
    "\\frac{\\delta}{\\delta\\mu}l(\\mu, \\sigma) = \\frac{1}{\\sigma^2}(\\sum_{i=1}^n x_i-n\\mu) = 0\\\\\n",
    "\\hat\\mu = \\frac{\\sum_{i=1}^n x_i}{n} = \\bar x\\\\\n",
    "\\\\\n",
    "\\frac{\\delta}{\\delta\\sigma}l(\\mu, \\sigma) = -\\frac{n}{\\sigma}+(\\sum_{i=1}^n (x_i-\\mu)^2)\\frac{1}{\\sigma^3} = 0\\\\\n",
    "\\sigma^2 = \\frac{\\sum_{i=1}^n (x_i-\\bar x)^2}{n}\\\\\n",
    "\\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i-\\bar x)^2}{n}}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian statistics**\n",
    "\n",
    "* $P(B)P(A|B) = P\\cap B$\n",
    "* Frequentists\n",
    "    * probabilities represent long term frequency of events\n",
    "    * concept of likelihood\n",
    "    * goal> find the model that most likely generated the observed data\n",
    "* Bayesians\n",
    "    * probabilities represent the degree of belief (or certainty)\n",
    "    * concept of prior\n",
    "    * goal> update prior belief based on observations\n",
    "\n",
    "Maximum a posteriori (MAP)\n",
    "* if a value of parameter is needed, we pick one with highest probability (the mode of updated belief), that is the posterior\n",
    "* with uniform prior belief, same results as with frequentist approach\n",
    "\n",
    "Updating priors  \n",
    "* $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n",
    "* A - event you are trying to predict, B - another event, or evidence, that will help refine the prediction\n",
    "* $P(A|B)$ is the posterior, belief that A will happen after observing evidence B\n",
    "* $P(A)$ is the prior, belief that A will happen, before observing the evidence B\n",
    "* $P(B|A)$ is the probability of evidence B appearing, given A happened\n",
    "* $P(B)$ is the probability of B in any circumstances, $P(B) = P(B|A)P(A)+P(B|A')P(A')$\n",
    "\n",
    "Discrete variables\n",
    "* $p_{Y|X=x}(y) = \\frac{p_{X|y=y}(x)p_Y(y)}{p_X(x)}$\n",
    "* Y - event to predict, X - informing event/evidence\n",
    "\n",
    "Continuos variables\n",
    "* $f_{Y|X=x}(y) = \\frac{f_{X|y=y}(x)f_Y(y)}{f_X(x)}$\n",
    "* Y - event to predict, X - informing event/evidence\n",
    "\n",
    "Combination of variables\n",
    "* use combination of PMF (discrete) and PDF (continuos)\n",
    "\n",
    "Fully worked Bernoulli Example\n",
    "* $\\Theta = P(Heads)$\n",
    "* $\\Theta$ is a continuous random variable\n",
    "* $X = (X_1,X_2,...,X_{10})$\n",
    "* $X_i=1$ if $H$, 0 if $T$\n",
    "* $H_i|\\Theta=\\theta\\sim Bernoulli(\\theta)$\n",
    "* $f_{\\Theta|X=x}(\\theta) = \\frac{p_{X|\\Theta=\\theta}(x)f_{\\Theta}(\\theta)}{p_X(x)}$\n",
    "* $p_{X|\\Theta=\\theta}(1,1,...,1,0,0)=\\theta^8(1-\\theta)^2$\n",
    "* $\\Theta\\sim Uniform(0,1)$\n",
    "Solution\n",
    "* $f_{\\Theta|X=x}(\\theta)=\\frac{\\theta^8(1-\\theta)^2 1}{constant} = \\frac{1}{constant}\\theta^8(1-\\theta)^2$\n",
    "* $f_{\\Theta|X=x}(\\theta)\\propto \\theta^8(1-\\theta)^2 1$\n",
    "    * constant can be ignored as we are searching for the maximum of the function, thus still need do find a derivative and equate it to 0\n",
    "\n",
    "Summary\n",
    "* Bayesians update priors\n",
    "* MAP with uninformative priors is just MLE\n",
    "* with enough data, MLE and MAP estimatecs converge\n",
    "* useful where limited data or strong prior beliefs\n",
    "* wrong priors, wrong conclusions\n",
    "\n",
    "**MAP, MLE and regularization**\n",
    "* simple vs complex model (higher prior) & poor vs great fit (high MLE), best fit - MAP\n",
    "* P(Model) - product of prob of points\n",
    "* cost func of a regularized model can be reformulated to MAP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
