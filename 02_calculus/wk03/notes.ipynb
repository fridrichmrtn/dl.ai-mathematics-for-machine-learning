{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in neural networks\n",
    "\n",
    "* finding internal weight and bias minimizing loss (distance between observed and predicted val)\n",
    "\n",
    "**Regression perceptron**\n",
    "* prediction func $\\^{y} = x_1 w_1 + x_2 w_2 +b$ (identity activation)\n",
    "* square loss $L(y,\\^{y}) = \\frac{1}{2}(y-\\^{y})^2$\n",
    "* gradient $\\nabla L = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\delta L}{\\delta w_1} \\\\\n",
    "    \\frac{\\delta L}{\\delta w_2}\\\\\n",
    "    \\frac{\\delta L}{\\delta b}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Appling chain rule\n",
    "\n",
    "$$\n",
    "\\frac{\\delta L}{\\delta b} =\n",
    "    \\frac{\\delta L}{\\delta \\^{y}} \\cdot \\frac{\\delta \\^{y}}{\\delta b}\n",
    "    = -(y-\\^{y})\\\\\n",
    "\\frac{\\delta L}{\\delta w_1} =\n",
    "    \\frac{\\delta L}{\\delta \\^{y}} \\cdot \\frac{\\delta \\^{y}}{\\delta w_1}\n",
    "    = -(y-\\^{y}) x_1\\\\\n",
    "\\frac{\\delta L}{\\delta w_2} =\n",
    "    \\frac{\\delta L}{\\delta \\^{y}} \\cdot \\frac{\\delta \\^{y}}{\\delta w_2}\n",
    "    = -(y-\\^{y}) x_2\n",
    "$$\n",
    "\n",
    "Move in the oposite direction of gradient using gradient descent approach.\n",
    "\n",
    "**Classification perceptron**\n",
    "* prediction func $\\^{y} = \\frac{1}{1+e^{-(x_1 w_1 + x_2 w_2 +b)}}$ (sigmoid activation)\n",
    "* sigmoid derivative $\\frac{d}{dz} \\sigma(z) = \\sigma(z) * (1-\\sigma(z))$ (see chain section)\n",
    "* log-loss $L(y,\\^{y}) = -y \\ln(\\^{y}) - (1-y) \\ln (1-\\^{y})$\n",
    "* gradient $\\nabla L = \n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\delta L}{\\delta w_1} \\\\\n",
    "    \\frac{\\delta L}{\\delta w_2}\\\\\n",
    "    \\frac{\\delta L}{\\delta b}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "Appling chain rule  \n",
    "\n",
    "$$\n",
    "\\frac{\\delta L}{\\delta b} =\n",
    "    \\frac{\\delta L}{\\delta \\^{y}} \\cdot \\frac{\\delta \\^{y}}{\\delta b}\n",
    "    = \\frac{-(y-\\^{y})}{\\^{y}(1-\\^{y})} \\^{y}(1-\\^{y})\n",
    "    = -(y-\\^{y})\\\\\n",
    "\\frac{\\delta L}{\\delta w_1} =\n",
    "    \\frac{\\delta L}{\\delta \\^{y}} \\cdot \\frac{\\delta \\^{y}}{\\delta w_1}\n",
    "    = \\frac{-(y-\\^{y})}{\\^{y}(1-\\^{y})} \\^{y}(1-\\^{y}) x_1\n",
    "    = -(y-\\^{y}) x_1\\\\\n",
    "\\frac{\\delta L}{\\delta w_2} =\n",
    "    \\frac{\\delta L}{\\delta \\^{y}} \\cdot \\frac{\\delta \\^{y}}{\\delta w_2}\n",
    "    = \\frac{-(y-\\^{y})}{\\^{y}(1-\\^{y})} \\^{y}(1-\\^{y}) x_2\n",
    "    = -(y-\\^{y}) x_2\n",
    "$$\n",
    "\n",
    "Move in the oposite direction of gradient using gradient descent approach.\n",
    "\n",
    "**Neural networks**\n",
    "* chaining the partial derivatives on the graph (example for a weight in the first layer of two-layer clf below), other params are calculated in a similar way \n",
    "\n",
    "$$\n",
    "\\frac{\\delta L}{\\delta w_{11}} = \n",
    "    \\frac{\\delta z_1}{\\delta w_{11}} \\cdot \\frac{\\delta a_1}{\\delta z_1}\n",
    "    \\cdot \\frac{\\delta\\^{y}}{\\delta a_1} \\cdot \\frac{\\delta \\^{y}}{\\delta z}\n",
    "    \\cdot \\frac{\\delta L}{\\delta\\^{y}} = \\\\\n",
    "    x_1 \\cdot a_1(1-a_1) \\cdot w_1 \\cdot \\^{y}(1-\\^{y}) \\cdot \\frac{-(y-\\^{y})}{\\^{y}(1-\\^{y})} = \\\\\n",
    "    -x_1w_1a_1(1-a_1)(y-\\^{y})\n",
    "$$\n",
    "\n",
    "# Newton's method\n",
    "\n",
    "**Univariate**\n",
    "\n",
    "* goal is to find zeros on a function, algoritm\n",
    "    * construct a tangent in a given point\n",
    "    * check intersection with x axis, project back onto the function\n",
    "    * repeat until you are in the zero point\n",
    "    * interative step $x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$\n",
    "\n",
    "* in optimization, we apply this on a derivative of a func\n",
    "* second derivative + (convex), -(concave), 0(line)\n",
    "* $f'(x) = 0, f''(x) > 0 \\rightarrow min$\n",
    "* $f'(x) = 0, f''(x) < 0 \\rightarrow max$\n",
    "* $f'(x) = 0, f''(x) = 0 \\rightarrow inconclusive$\n",
    "\n",
    "**Hessian**\n",
    "* multivariable optimization, partial derivatives along the dimensions\n",
    "* $\n",
    "H = \\begin{bmatrix}\n",
    "    \\frac{\\delta^2 f(x,y)}{\\delta^2 x} & \\frac{\\delta^2 f(x,y)}{\\delta x \\delta y}\\\\\n",
    "    \\frac{\\delta^2 f(x,y)}{\\delta y \\delta x}& \\frac{\\delta^2 f(x,y)}{\\delta^2 y}\\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "* for understanding the curvarture of the shape, you need to get eigen-values $det(H(x_{0},y_{0})-\\lambda I)$\n",
    "* convex - eigen-values>0, convex - eigen-values<0, eigen-values mixed - saddle\n",
    "\n",
    "**Multivariate**\n",
    "\n",
    "* iterative step $\n",
    "\\begin{bmatrix} x_{k+1}\\\\y_{k+1}\\end{bmatrix} =\n",
    "\\begin{bmatrix} x_{k}\\\\y_{k}\\end{bmatrix} - H^{-1}(x_k, y_k) \\nabla (x_k,y_k)\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., 10.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linalg.eigvals(np.array([[2,0],[0,10]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
